# 功能规格说明：Watermill 消息中间件 SDK

**功能分支**: `001-watermill-messaging-sdk`
**创建日期**: 2026-01-30
**状态**: 草稿
**输入**: 用户描述："实现一个基于 Watermill 封装的、集成于 go-zero 生态的生产级消息中间件 SDK，它利用 Redis Streams 实现了轻量且可靠的异步通信（支持持久化、重试与死信队列），并通过接口隔离确保了未来向 Redpanda（kafka） 的易修改"

## 用户场景与测试 *(必填)*

### 用户故事 1 - 基础消息发布与消费 (优先级: P1)

作为后端开发者，我需要向主题发布消息并让消费者可靠地接收它们，以便实现微服务之间的异步通信。

**优先级原因**: 这是任何消息系统的核心功能。没有可靠的发布/订阅，其他功能都无从谈起。这是实现事件驱动架构模式的基础。

**独立测试**: 可以通过向主题发布消息并验证消费者在预期时间内（< 100ms）接收到消息来完整测试。通过启用基础异步通信立即交付价值。

**验收场景**:

1. **假设** 发布者配置了 Redis Streams 后端，**当** 向主题 "user.registered" 发布消息时，**那么** 消息持久化到 Redis 并返回成功
2. **假设** 消费者订阅了主题 "user.registered"，**当** 向该主题发布消息时，**那么** 消费者在 100ms 内接收到消息
3. **假设** 同一消费者组中有多个消费者，**当** 发布消息时，**那么** 消息在消费者之间均匀分配（负载均衡）

---

### 用户故事 2 - 指数退避自动重试 (优先级: P2)

作为后端开发者，我需要失败的消息处理能够以递增的延迟自动重试，以便瞬时故障不会导致消息丢失。

**优先级原因**: 重试逻辑对生产环境的可靠性至关重要，但可以在基础发布/订阅功能之后实现。它显著提高了系统对临时故障的弹性。

**独立测试**: 可以通过模拟一个失败 N 次然后成功的消费者来测试。验证重试尝试以正确的退避间隔发生，消息最终成功。

**验收场景**:

1. **假设** 消费者处理消息失败，**当** 检测到失败时，**那么** 消息在 1 秒后自动重试
2. **假设** 消息已失败两次，**当** 第三次重试发生时，**那么** 重试延迟为 4 秒（指数退避：1秒、2秒、4秒）
3. **假设** 消息已重试 3 次（可配置的最大值），**当** 再次失败时，**那么** 消息被移动到死信队列

---

### 用户故事 3 - 失败消息的死信队列 (优先级: P2)

作为后端开发者，我需要在所有重试后仍然失败的消息被移动到死信队列，以便我可以调查并手动重新处理它们，而不会阻塞主队列。

**优先级原因**: 死信队列防止有毒消息阻塞队列，并提供故障的可见性。对生产运维至关重要，但依赖于重试机制。

**独立测试**: 可以通过发布一个总是失败的消息来测试，验证它在最大重试次数后移动到死信队列，并确认主队列继续处理其他消息。

**验收场景**:

1. **假设** 消息已超过最大重试次数，**当** 最后一次重试失败时，**那么** 消息被移动到主题 "{原始主题}.dlq"
2. **假设** 消息在死信队列中，**当** 运维人员查询死信队列时，**那么** 消息包含原始负载、错误详情和重试历史
3. **假设** 消息在死信队列中，**当** 运维人员手动重新处理它时，**那么** 消息被移回原始主题进行重新处理

---

### 用户故事 4 - Go-Zero 中间件集成 (优先级: P3)

作为使用 go-zero 的后端开发者，我需要消息 SDK 与 go-zero 的服务上下文和中间件无缝集成，以便我可以利用现有的日志、追踪和监控基础设施。

**优先级原因**: 与 go-zero 生态的集成改善了开发者体验和运维可见性，但 SDK 可以先独立运行。

**独立测试**: 可以通过在 go-zero 服务中发布/消费消息并验证 trace ID 传播、日志包含服务上下文、指标被收集来测试。

**验收场景**:

1. **假设** 从 go-zero 服务发布消息，**当** 消息被消费时，**那么** 原始请求上下文中的 trace_id 被保留
2. **假设** 消费者正在处理消息，**当** 发生错误时，**那么** 结构化日志包含服务名称、trace_id 和消息元数据
3. **假设** 消息正在被处理，**当** 查询监控指标时，**那么** 指标包含每个主题的消息数量、处理延迟和错误率

---

### 用户故事 5 - 后端抽象以支持未来迁移 (优先级: P3)

作为平台架构师，我需要 SDK 将底层消息代理抽象在接口之后，以便我们可以从 Redis Streams 迁移到 Redpanda/Kafka 而无需更改应用代码。

**优先级原因**: 面向未来对长期可维护性很重要，但不提供即时价值。可以在核心功能稳定后实现。

**独立测试**: 可以通过实现一个模拟后端（内存）并验证所有 SDK 功能工作相同来测试。之后，实现 Redpanda 后端并运行相同的测试套件。

**验收场景**:

1. **假设** SDK 配置了 Redis Streams 后端，**当** 应用代码发布/消费消息时，**那么** 不向应用代码暴露 Redis 特定的类型或方法
2. **假设** 有新的 Redpanda 后端实现，**当** 配置更改为使用 Redpanda 时，**那么** 现有应用代码无需修改即可工作
3. **假设** 有多个后端实现，**当** 切换后端时，**那么** 只需要配置更改（无需代码更改）

---

### 边界情况

- 当消息发布期间 Redis 连接丢失时会发生什么？（应在内存中缓冲消息直到达到限制，然后返回错误）
- 系统如何处理大于 Redis Stream 条目大小限制（通常为 512MB）的消息？（应拒绝并返回清晰的错误消息指示大小限制）
- 当消费者比消息生产速率慢时会发生什么？（应使用消费者组进行负载均衡；如果所有消费者都慢，消息在 Redis 中累积并产生背压）
- 系统如何处理重复的消息 ID？（Redis Streams 自然按 ID 去重；SDK 应使用时间戳 + 随机后缀生成唯一 ID）
- 当死信队列本身失败时会发生什么？（应记录错误并暴露指标；运维人员必须监控死信队列健康状况）
- 优雅关闭期间如何处理消息？（应在可配置的超时时间内完成处理中的消息，然后关闭连接）

## 需求 *(必填)*

### 功能需求

- **FR-001**: 系统必须提供一个 Publisher 接口，接受主题名称和消息负载（字节切片或结构体）
- **FR-002**: 系统必须提供一个 Subscriber 接口，接受主题名称、消费者组和消息处理函数
- **FR-003**: 系统必须将消息持久化到 Redis Streams，具有可配置的保留期（默认 7 天）
- **FR-004**: 系统必须支持消费者组，以便在多个实例之间进行负载均衡的消息消费
- **FR-005**: 系统必须使用指数退避自动重试失败的消息（可配置：初始延迟、最大延迟、最大尝试次数）
- **FR-006**: 系统必须在超过最大重试次数后将消息移动到死信队列
- **FR-007**: 系统必须在单个分区/分片内为有序主题保留消息顺序
- **FR-008**: 系统必须在消息发布和消费操作之间传播 trace_id 和 correlation_id
- **FR-009**: 系统必须暴露每个主题的消息数量、处理延迟、错误率和重试次数的指标
- **FR-010**: 系统必须支持优雅关闭，为处理中的消息提供可配置的超时时间
- **FR-011**: 系统必须将后端实现抽象在接口之后（Publisher、Subscriber、Backend）
- **FR-012**: 系统必须提供 Redis 连接的配置选项（地址、密码、数据库、连接池大小）
- **FR-013**: 系统必须支持消息序列化格式：JSON（默认）、Protocol Buffers 和自定义序列化器
- **FR-014**: 系统必须验证消息负载大小，并拒绝超过后端限制的消息
- **FR-015**: 系统必须提供死信队列管理操作：列出、检查、重新处理和删除消息

### 关键实体

- **消息（Message）**: 表示要传输的数据单元，包含负载（字节）、元数据（主题、时间戳、trace_id、correlation_id）和传递信息（尝试次数、错误历史）
- **主题（Topic）**: 用于消息路由的命名通道，具有保留、顺序保证和重试策略的配置
- **消费者组（Consumer Group）**: 共享消息处理负载的消费者的逻辑分组，确保组中只有一个消费者处理每条消息
- **死信队列（Dead Letter Queue）**: 存储在所有重试尝试后处理失败的消息的特殊主题，带有用于调试的额外元数据
- **后端（Backend）**: 底层消息代理实现（Redis Streams、Redpanda 等），抽象在接口之后
- **发布者（Publisher）**: 负责向主题发送消息的组件，处理序列化和后端特定的发布逻辑
- **订阅者（Subscriber）**: 负责从主题接收消息的组件，处理反序列化、重试逻辑和调用用户定义的处理器

## 成功标准 *(必填)*

### 可度量的结果

- **SC-001**: 在正常负载下（每个主题 < 1000 消息/秒），消息在 100ms 内传递给消费者
- **SC-002**: 系统在单个 Redis 实例上处理所有主题至少 10,000 条消息/秒
- **SC-003**: 失败的消息在配置的间隔内（1秒、2秒、4秒）自动重试，重试后成功率为 99%
- **SC-004**: 正常操作期间零消息丢失（消息在发布返回成功之前持久化）
- **SC-005**: 优雅关闭在 30 秒内完成，处理所有处理中的消息
- **SC-006**: 死信队列消息在失败后 1 分钟内可供检查和重新处理
- **SC-007**: SDK 与 go-zero 服务集成，配置代码少于 10 行
- **SC-008**: 后端可以通过仅更改配置从 Redis 切换到 Redpanda（零代码更改）
- **SC-009**: 消息处理延迟 P99 < 200ms（从发布到处理器完成）
- **SC-010**: 系统保持 99.9% 的正常运行时间，在瞬时故障时自动重新连接

## 假设

- Redis Streams 可用并正确配置了持久化（AOF 或 RDB）
- 服务与 Redis 之间的网络延迟 < 10ms
- 消息负载通常 < 1MB（较大的消息应使用对象存储，消息包含引用）
- 消费者处理器是幂等的（可以安全地多次处理同一消息）
- Go-Zero 框架版本与 SDK 兼容（go-zero v1.5+）
- 开发者熟悉基本的发布/订阅消息模式
- Redis 实例有足够的内存用于消息保留期（根据消息速率和大小估算）
- 监控基础设施（Prometheus）可用于指标收集

## 依赖项

- **Watermill**: 核心消息抽象库 (github.com/ThreeDotsLabs/watermill)
- **Go-Zero**: 用于集成的微服务框架 (github.com/zeromq/goczmq)
- **Redis**: 消息代理后端 (github.com/redis/go-redis)
- **OpenTelemetry**: 分布式追踪 (go.opentelemetry.io/otel)
- **Prometheus**: 指标收集 (github.com/prometheus/client_golang)

## 超出范围

- 静态消息加密（应由 Redis 配置处理）
- 多数据中心复制（Redis 集群配置责任）
- 消息模式验证（应由应用层处理）
- 死信队列管理的 Web UI（CLI 工具或单独的管理服务）
- 消息优先级支持（Redis Streams 本身不支持此功能）
- 跨多个主题的事务性消息发布（需要分布式事务协调器）
- 基于内容的消息过滤/路由（应由多个主题或应用逻辑处理）
